---
title: Algorithmic Stability of Heavy-Tailed Stochastic Gradient Descent on Least
  Squares
abstract: 'Recent studies have shown that heavy tails can  emerge in stochastic optimization
  and that the heaviness of the tails have links to the generalization error. While
  these studies have shed light on interesting aspects of the generalization behavior
  in modern settings, they relied on strong topological and statistical regularity
  assumptions, which are hard to verify in practice. Furthermore, it has been empirically
  illustrated that the relation between heavy tails and generalization might not always
  be monotonic in practice, contrary to the conclusions of existing theory. In this
  study, we establish novel links between the tail behavior and generalization properties
  of stochastic gradient descent (SGD), through the lens of algorithmic stability.
  We consider a quadratic optimization problem and use a heavy-tailed stochastic differential
  equation (and its Euler discretization) as a proxy for modeling the heavy-tailed
  behavior emerging in SGD. We then prove uniform stability bounds, which reveal the
  following outcomes: (i) Without making any exotic assumptions, we show that SGD
  will not be stable if the stability is measured with the squared-loss $x\mapsto
  x^2$, whereas it in turn becomes stable if the stability is instead measured with
  a surrogate loss $x\mapsto |x|^p$ with some $p<2$. (ii) Depending on the variance
  of the data, there exists a \emph{‘threshold of heavy-tailedness’} such that the
  generalization error decreases as the tails become heavier, as long as the tails
  are lighter than this threshold. This suggests that the relation between heavy tails
  and generalization is not globally monotonic. (iii) We prove matching lower-bounds
  on uniform stability, implying that our bounds are tight in terms of the heaviness
  of the tails. We support our theory with synthetic and real neural network experiments.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: raj23a
month: 0
tex_title: Algorithmic Stability of Heavy-Tailed Stochastic Gradient Descent on Least
  Squares
firstpage: 1292
lastpage: 1342
page: 1292-1342
order: 1292
cycles: false
bibtex_author: Raj, Anant and Barsbey, Melih and Gurbuzbalaban, Mert and Zhu, Lingjiong
  and \c{S}im\s{c}ekli, Umut
author:
- given: Anant
  family: Raj
- given: Melih
  family: Barsbey
- given: Mert
  family: Gurbuzbalaban
- given: Lingjiong
  family: Zhu
- given: Umut
  family: Şim\scekli
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/raj23a/raj23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
