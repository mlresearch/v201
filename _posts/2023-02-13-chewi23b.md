---
title: Fisher information lower bounds for sampling
abstract: "  We prove two lower bounds for the complexity of non-log-concave sampling
  within the framework of Balasubramanian et al. (2022), who introduced the use of
  Fisher information ($\\mathsf{FI}$) bounds as a notion of approximate first-order
  stationarity in sampling. Our first lower bound shows that averaged Langevin Monte
  Carlo (LMC) is optimal for the regime of large $\\mathsf{FI}$ by reducing the problem
  of finding stationary points in non-convex optimization to sampling. Our second
  lower bound shows that in the regime of small $\\mathsf{FI}$, obtaining a $\\mathsf{FI}$
  of at most $\\varepsilon^2$ from the target distribution requires $\\text{poly}(1/\\varepsilon)$
  queries, which is surprising as it rules out the existence of high-accuracy algorithms
  (e.g., algorithms using Metropolis{â€“}Hastings filters) in this context."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chewi23b
month: 0
tex_title: Fisher information lower bounds for sampling
firstpage: 375
lastpage: 410
page: 375-410
order: 375
cycles: false
bibtex_author: Chewi, Sinho and Gerber, Patrik and Lee, Holden and Lu, Chen
author:
- given: Sinho
  family: Chewi
- given: Patrik
  family: Gerber
- given: Holden
  family: Lee
- given: Chen
  family: Lu
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/chewi23b/chewi23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
