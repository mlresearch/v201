---
title: Linear Reinforcement Learning with Ball Structure Action Space
abstract: We study the problem of Reinforcement Learning (RL) with linear function
  approximation, i.e. assuming the optimal action-value function is linear in a known
  $d$-dimensional feature mapping. Unfortunately, however, based on only this assumption,
  the worst case sample complexity has been shown to be exponential, even under a
  generative model. Instead of making further assumptions on the MDP or value functions,
  we assume that our action space is such that there always exist playable actions
  to explore any direction of the feature space. We formalize this assumption as a
  “ball structure” action space, and show that being able to freely explore the feature
  space allows for efficient RL. In particular, we propose a sample-efficient RL algorithm
  (BallRL) that learns an $\epsilon$-optimal policy using only $\tilde{\mathcal{O}}\left(\frac{H^5d^3}{\epsilon^3}\right)$
  number of trajectories.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jia23a
month: 0
tex_title: Linear Reinforcement Learning with Ball Structure Action Space
firstpage: 755
lastpage: 775
page: 755-775
order: 755
cycles: false
bibtex_author: Jia, Zeyu and Jia, Randy and Madeka, Dhruv and Foster, Dean P.
author:
- given: Zeyu
  family: Jia
- given: Randy
  family: Jia
- given: Dhruv
  family: Madeka
- given: Dean P.
  family: Foster
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/jia23a/jia23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
