---
title: Limitations of Information-Theoretic Generalization Bounds for Gradient Descent
  Methods in Stochastic Convex Optimization
abstract: 'To date, no “information-theoretic” frameworks for reasoning about generalization
  error have been shown to establish minimax rates for gradient descent in the setting
  of stochastic convex optimization. In this work, we consider the prospect of establishing
  such rates via several existing information-theoretic frameworks: input-output mutual
  information bounds, conditional mutual information bounds and variants, PAC-Bayes
  bounds, and recent conditional variants thereof. We prove that none of these bounds
  are able to establish minimax rates. We then consider a common tactic employed in
  studying gradient methods, whereby the final iterate is corrupted by Gaussian noise,
  producing a noisy “surrogate” algorithm. We prove that minimax rates cannot be established
  via the analysis of such surrogates. Our results suggest that new ideas are required
  to analyze gradient descent using information-theoretic techniques.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: haghifam23a
month: 0
tex_title: Limitations of Information-Theoretic Generalization Bounds for Gradient
  Descent Methods in Stochastic Convex Optimization
firstpage: 663
lastpage: 706
page: 663-706
order: 663
cycles: false
bibtex_author: Haghifam, Mahdi and Rodr\'iguez-G\'alvez, Borja and Thobaben, Ragnar
  and Skoglund, Mikael and M. Roy, Daniel and Karolina Dziugaite, Gintare
author:
- given: Mahdi
  family: Haghifam
- given: Borja
  family: Rodríguez-Gálvez
- given: Ragnar
  family: Thobaben
- given: Mikael
  family: Skoglund
- given: Daniel
  family: M. Roy
- given: Gintare
  family: Karolina Dziugaite
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/haghifam23a/haghifam23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
