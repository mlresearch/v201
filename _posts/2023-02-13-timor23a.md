---
title: Implicit Regularization Towards Rank Minimization in ReLU Networks
abstract: We study the conjectured relationship between the implicit regularization
  in neural networks, trained with gradient-based methods, and rank minimization of
  their weight matrices. Previously, it was proved that for linear networks (of depth $2$
  and vector-valued outputs), gradient flow (GF) w.r.t. the square loss acts as a
  rank minimization heuristic. However, understanding to what extent this generalizes
  to nonlinear networks is an open problem. In this paper, we focus on nonlinear ReLU networks,
  providing several new positive and negative results. On the negative side, we prove
  (and demonstrate empirically) that, unlike the linear case, GF on ReLU networks
  may no longer tend to minimize ranks, in a rather strong sense (even approximately,
  for “most” datasets of size $2$). On the positive side, we reveal that ReLU networks
  of sufficient depth are provably biased towards low-rank solutions in several reasonable
  settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: timor23a
month: 0
tex_title: Implicit Regularization Towards Rank Minimization in ReLU Networks
firstpage: 1429
lastpage: 1459
page: 1429-1459
order: 1429
cycles: false
bibtex_author: Timor, Nadav and Vardi, Gal and Shamir, Ohad
author:
- given: Nadav
  family: Timor
- given: Gal
  family: Vardi
- given: Ohad
  family: Shamir
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/timor23a/timor23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
