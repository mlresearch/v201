---
title: Online Self-Concordant and Relatively Smooth Minimization, With Applications
  to Online Portfolio Selection and Learning Quantum States
abstract: 'This work considers online portfolio selection (OPS) and online learning
  quantum states with the logarithmic loss. The problem of designing an optimal OPS
  algorithm, in both regret and efficiency, has been open for more than 30 years \citep{Cover1991,Cover1996,Helmbold1998,Nesterov2011,Orseau2017,Luo2018,van-Erven2020,Mhammedi2022,Zimmert2022}.
  Online learning quantum states is a generalization of OPS to the quantum setup \citep{Lin2021b,Zimmert2022}.
  The dimension of a quantum state grows exponentially with the number of qubits,
  so scalability with respect to the dimension becomes a critical issue in the quantum
  setup. We formulate the two problems as online convex optimization where the loss
  functions are self-concordant barriers and smooth relative to a convex function
  $h$. We analyze the regret of online mirror descent with $h$ as the regularizer.
  Then, based on the analysis, we prove the following in a \emph{unified} manner.
  Denote by $T$ the time horizon and $d$ the parameter dimension. \begin{enumerate}[leftmargin=0.5in,rightmargin=0.3in]
  \item For OPS, the regret of \tildeEG{}, a variant of exponentiated gradient due
  to \citet{Helmbold1998}, is $\tilde{O} ( T^{2/3} d^{1/3} )$. This improves on the
  original $\tilde{O} ( T^{3/4} d^{1/2} )$ regret bound for \tildeEG{}. \item For
  OPS, the regret of LB-OMD (online mirror descent with the logarithmic barrier) is
  $\tilde{O}(\sqrt{T d})$. The regret bound and the per-iteration time are the same
  as those of Soft-Bayes due to \citet{Orseau2017} up to logarithmic terms. \item
  For online learning quantum states with the logarithmic loss, the regret of online
  mirror descent with the log-determinant function is also $\tilde{O} ( \sqrt{T d}
  )$. Its per-iteration time is $O(d^3)$, shorter than all existing algorithms we
  know. \end{enumerate} FigureÂ \ref{fig_frontier} presents the current efficiency-regret
  Pareto frontier for OPS, where the two results in bold are our contributions. In
  contrast to existing works on online convex optimization \citep{Nesterov2011,Abernethy2012,van-Erven2020},
  our analysis does not assume $h$ to be a self-concordant barrier. Instead, we exploit
  the self-concordant barrier property of the \textit{loss functions}, and only require
  $h$ to satisfy the relative smooth property. One advantage of our approach is that
  it includes \tildeEG{} as an instance, though the entropic regularizer is not a
  barrier function. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tsai23a
month: 0
tex_title: Online Self-Concordant and Relatively Smooth Minimization, With Applications
  to Online Portfolio Selection and Learning Quantum States
firstpage: 1481
lastpage: 1483
page: 1481-1483
order: 1481
cycles: false
bibtex_author: Tsai, Chung-En and Cheng, Hao-Chung and Li, Yen-Huan
author:
- given: Chung-En
  family: Tsai
- given: Hao-Chung
  family: Cheng
- given: Yen-Huan
  family: Li
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/tsai23a/tsai23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
