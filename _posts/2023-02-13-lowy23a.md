---
title: 'Private Stochastic Optimization with Large Worst-Case Lipschitz Parameter:
  Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses'
abstract: We study differentially private (DP) stochastic optimization (SO) with loss
  functions whose worst-case Lipschitz parameter over all data points may be extremely
  large. To date, the vast majority of work on DP SO assumes that the loss is uniformly
  Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded  over
  all data points). While this assumption is convenient, it often leads to pessimistic
  excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz
  parameter of the loss over all data points may be extremely large due to outliers.
  In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz
  parameter of the loss, are vacuous. To address these limitations, this work provides
  near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter
  of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al.,
  2022), we assume that stochastic gradients have bounded $k$-th order \textit{moments}
  for some $k \geq 2$. Compared with works on uniformly Lipschitz DP SO, our excess
  risk scales with the $k$-th moment bound instead of the uniform Lipschitz parameter
  of the loss, allowing for significantly faster rates in the presence of outliers
  and/or heavy-tailed data.  For \textit{convex} and \textit{strongly convex} loss
  functions, we provide the first asymptotically \textit{optimal} excess risk bounds
  (up to a logarithmic factor). In contrast to (Wang et al., 2020; Kamath et al.,
  2022), our bounds do not require the loss function to be differentiable/smooth.
  We also devise an accelerated algorithm for smooth losses that runs in linear time
  and has excess risk that is tight in certain practical parameter regimes. Additionally,
  our work is the first to address \textit{non-convex} non-uniformly Lipschitz loss
  functions satisfying the \textit{Proximal-PL inequality}; this covers some practical
  machine learning models. Our Proximal-PL algorithm has near-optimal excess risk.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lowy23a
month: 0
tex_title: 'Private Stochastic Optimization with Large Worst-Case Lipschitz Parameter:
  Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses'
firstpage: 986
lastpage: 1054
page: 986-1054
order: 986
cycles: false
bibtex_author: Lowy, Andrew and Razaviyayn, Meisam
author:
- given: Andrew
  family: Lowy
- given: Meisam
  family: Razaviyayn
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/lowy23a/lowy23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
