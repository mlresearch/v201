---
title: Variance-Reduced Conservative Policy Iteration
abstract: We study the sample complexity of reducing reinforcement learning to a sequence
  of empirical risk minimization problems over the policy space. Such reductions-based
  algorithms exhibit local convergence in the function space, as opposed to the parameter
  space for policy gradient algorithms, and thus are unaffected by the possibly non-linear
  or discontinuous parameterization of the policy class. We propose a variance-reduced
  variant of Conservative Policy Iteration that improves the sample complexity of
  producing a $\varepsilon$-functional local optimum from $O(\varepsilon^{-4})$ to
  $O(\varepsilon^{-3})$. Under state-coverage and policy-completeness assumptions,
  the algorithm enjoys $\varepsilon$-global optimality after sampling $O(\varepsilon^{-2})$
  times, improving upon the previously established $O(\varepsilon^{-3})$ sample requirement.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwal23a
month: 0
tex_title: Variance-Reduced Conservative Policy Iteration
firstpage: 3
lastpage: 33
page: 3-33
order: 3
cycles: false
bibtex_author: Agarwal, Naman and Bullins, Brian and Singh, Karan
author:
- given: Naman
  family: Agarwal
- given: Brian
  family: Bullins
- given: Karan
  family: Singh
date: 2023-02-13
address:
container-title: Proceedings of The 34th International Conference on Algorithmic Learning
  Theory
volume: '201'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 2
  - 13
pdf: https://proceedings.mlr.press/v201/agarwal23a/agarwal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
